{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import signal\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tkinter import Tk,Frame,Canvas,Text,Button,CENTER,WORD\n",
    "from PIL import ImageTk,Image\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import filecmp\n",
    "import shutil\n",
    "\n",
    "global temp_folder\n",
    "temp_folder = sys.path[0] + '/Data/temp'\n",
    "\n",
    "# Creating timeout error to limit the runtime of a function if needed\n",
    "class TimeoutException(Exception): # Creating custom error\n",
    "    pass\n",
    "\n",
    "def timeout_handler(): # Creating function to handle error\n",
    "    raise TimeoutException\n",
    "    \n",
    "signal.signal(signal.SIGALRM, timeout_handler);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    '''\n",
    "    This function reads the html and finds the title, returning it as a string\n",
    "    '''\n",
    "    if (url == None) or ('http' not in url):\n",
    "        raise ValueError(f'Not a useable url: \"{url}\"')\n",
    "    try:\n",
    "        response = urlopen(url)\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        title = soup.title.get_text()\n",
    "        del url,soup,response\n",
    "        return title\n",
    "    except:\n",
    "        raise ValueError(f'Not a useable url: \"{url}\"')\n",
    "    \n",
    "def find_all_links(page):\n",
    "    '''\n",
    "    Finds all links on page, this is a WIP as it doesn't find all links\n",
    "    '''\n",
    "    soup = BeautifulSoup(page,features=\"lxml\")\n",
    "    all_links = []\n",
    "    for line in soup.find_all('a'):\n",
    "        line = line.get('href')\n",
    "        try:\n",
    "            if ('http' not in line):\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        all_links.append(line)\n",
    "    del soup,line,page\n",
    "    return list(set(all_links))\n",
    "\n",
    "def find_relevant_links_and_titles(all_links,keywords,search_title = False):\n",
    "    '''\n",
    "    Given a list of links and a list of keywords this method will find links/titles that contain\n",
    "    keywords\n",
    "\n",
    "    all_links: list, contains all links you want to parse\n",
    "\n",
    "    keywords: list, contains all keywords you want to look for\n",
    "\n",
    "    search_title: bool, leave this set to false in order to save time, but if you want to search\n",
    "        titles anyway, set it to true. If set to true, the script will download the html\n",
    "        of the page, strip the title from it, and then look for keywords. This is a massive\n",
    "        time waste.\n",
    "    '''\n",
    "    name_and_link = []\n",
    "    keywords_upper = [string.capitalize() for string in keywords]\n",
    "    for link in all_links:\n",
    "        signal.alarm(5)\n",
    "        if search_title:\n",
    "            try:\n",
    "                title = get_title(link).lower()\n",
    "                if any(substring in title for substring in keywords):\n",
    "                    name_and_link.append((title,link)) \n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                continue\n",
    "            else: \n",
    "                signal.alarm(0)\n",
    "        else:\n",
    "            try:\n",
    "                link = link.lower()\n",
    "                if any(substring in link for substring in keywords):\n",
    "                    title = get_title(link)\n",
    "                    name_and_link.append((title,link))\n",
    "                else:              \n",
    "                    pass\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                signal.alarm(0)\n",
    "    del keywords,keywords_upper,link,all_links\n",
    "    return list(set(name_and_link))\n",
    "\n",
    "def relevant_links(url,keywords):\n",
    "    '''\n",
    "    This method takes a link, strips all links on the page, and finds all links that contain\n",
    "    keywords\n",
    "\n",
    "    url: string, link to website you want to parse\n",
    "\n",
    "    keywords: list, contains all keywords to look for\n",
    "\n",
    "    returns: list of links that match\n",
    "    '''\n",
    "\n",
    "    page = urlopen(url).read()\n",
    "\n",
    "    all_links = find_all_links(page)\n",
    "\n",
    "    name_and_link = list(set(find_relevant_links_and_titles(all_links,keywords,search_title=False)))\n",
    "\n",
    "    return name_and_link\n",
    "\n",
    "def add_to_dataframe(df,name_link_type):\n",
    "    '''\n",
    "    This method can be used to add/save websites to the website csv, returns dataframe with new \n",
    "    entries added to end of frame.\n",
    "\n",
    "    df: This is the dataframe you want to add the entries to.\n",
    "\n",
    "    name_link_type: This is an array that contains tuples with title of a website in the first position\n",
    "    the link in the second position, and the type of each dataset. \n",
    "    Should Look like this -> \n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    del df\n",
    "    for tup in name_link_type:\n",
    "        new_df.loc[len(new_df.index)] = [tup[0], tup[1], tup[2], round(time.time()), \"empty\"]\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def remove_entry(df,idx,save_immediately=False):\n",
    "    '''\n",
    "    This method deletes an entry by index, saves changes immediately depending on you choice, and then returns the new dataframe\n",
    "    If save_immediately is set to false, then if the program is unable to reach then end of the main loop, the change will not be saved,\n",
    "    \n",
    "\n",
    "    df: Dataframe you are working with\n",
    "\n",
    "    idx: integer, index of the row/entry you want deleted\n",
    "\n",
    "    returns: New dataframe with entry removed\n",
    "    '''\n",
    "    new_df = df.copy()\n",
    "    del df\n",
    "\n",
    "    new_df = new_df.drop(idx).reset_index(drop=True)\n",
    "\n",
    "    if save_immediately:\n",
    "        new_df.to_csv('websites.csv',index=False)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def on_start():\n",
    "   global run\n",
    "   run = True\n",
    "\n",
    "def on_stop():\n",
    "   global run\n",
    "   run = False\n",
    "\n",
    "def connected_to_internet(url='http://www.google.com/', timeout=5):\n",
    "    try:\n",
    "        _ = requests.head(url, timeout=timeout)\n",
    "        return True\n",
    "    except requests.ConnectionError:\n",
    "      return False\n",
    "\n",
    "def download_url(url, save_path, chunk_size=1024, type='csv'):\n",
    "   '''\n",
    "   Save path is just the folder you want to download it in.\n",
    "\n",
    "   Type must be a string, with the type of file you're downloading\n",
    "\n",
    "   returns name of new file and its filepath. If downloaded is a zip, it will extract it and then \n",
    "   return the path to the folder along with the name of the folder\n",
    "   '''\n",
    "   files_in_directory = len(next(os.walk(save_path), (None, None, []))[2])\n",
    "   name = save_path.split(sep='/')[-1] + '-' + str(files_in_directory) + '.' + type\n",
    "   filepath = save_path + '/' + name\n",
    "\n",
    "   r = requests.get(url, stream=True)\n",
    "   with open(save_path + '/' + name, 'wb') as fd:\n",
    "      for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "         fd.write(chunk)\n",
    "   \n",
    "   if type == 'zip':\n",
    "      with ZipFile(save_path + '/' + name, 'r') as zObject:  \n",
    "         zObject.extractall(path=save_path + '/')\n",
    "         zObject.close()\n",
    "         os.unlink(filepath)\n",
    "      filepath = max(glob.glob(os.path.join(save_path, '*/')), key=os.path.getmtime)\n",
    "      name = filepath.split(sep='/')[-2]\n",
    "\n",
    "      return filepath\n",
    "\n",
    "   return filepath\n",
    "\n",
    "def clear_temp(dir=temp_folder):\n",
    "   '''\n",
    "   Clears all files and directories from temp folder\n",
    "   '''\n",
    "   for filename in os.listdir(dir):\n",
    "      file_path = os.path.join(dir, filename)\n",
    "      try:\n",
    "         if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                  os.unlink(file_path)\n",
    "         elif os.path.isdir(file_path):\n",
    "                  shutil.rmtree(file_path)\n",
    "      except:\n",
    "         pass\n",
    "def autoprocess(data_path,dl_folder):\n",
    "   try:\n",
    "      data = pd.read_csv(data_path,low_memory=False)\n",
    "   except:\n",
    "      try:\n",
    "         filename = data_path.split(sep='/')[-1]\n",
    "         data_folder = dl_folder + '/' + filename.split(sep='.')[0]\n",
    "         if not os.path.isdir(data_folder):\n",
    "            os.mkdir(data_folder)\n",
    "         failed_to_process = open(data_folder + '/failed_to_process.txt','a')\n",
    "         e = datetime.now()\n",
    "         failed_to_process.write(f'{filename} failed to open on {str(e.year)}, {str(e.month)}, {str(e.day)}\\n')\n",
    "         failed_to_process.close()\n",
    "         return\n",
    "      except:\n",
    "         return\n",
    "\n",
    "   filename = data_path.split(sep='/')[-1].split(sep='.')[0]\n",
    "   data_folder = dl_folder + '/' + filename.split(sep='.')[0] + '_Data'\n",
    "   if not os.path.isdir(data_folder):\n",
    "      os.mkdir(data_folder)\n",
    "      os.mkdir(data_folder + '/Histograms')\n",
    "      os.mkdir(data_folder + '/Plots')\n",
    "\n",
    "   failed1 = [False for idx,col in enumerate(data) if (data.dtypes[idx] != 'object') and (data.dtypes[idx] != 'bool')]\n",
    "   for idx,col in enumerate(data):\n",
    "      if (data.dtypes[idx] != 'object') and (data.dtypes[idx] != 'bool'):\n",
    "         signal.alarm(5)\n",
    "         try:            \n",
    "            fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\n",
    "            ax.hist(np.array(data[col]))\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_title('Autogenerated Histogram' + ' ' + str(idx + 1))\n",
    "            ax.set_facecolor('#ADD8E6')\n",
    "            ax.set_axisbelow(True)\n",
    "            ax.yaxis.grid(color='white', linestyle='-')\n",
    "            fig.savefig(data_folder + '/Histograms/' + filename + '_' + 'Histogram-' + str(idx + 1) + '.png')\n",
    "            plt.close('all')\n",
    "         except:\n",
    "            plt.close('all')\n",
    "            failed1[idx] = True\n",
    "            pass\n",
    "         else:\n",
    "            signal.alarm(0)\n",
    "   \n",
    "   failed2 = [False for idx,col in enumerate(data) if (data.dtypes[idx] != 'object') and (data.dtypes[idx] != 'bool')]\n",
    "   for idx,col in enumerate(data):\n",
    "      if (data.dtypes[idx] != 'object') and (data.dtypes[idx] != 'bool'):\n",
    "         signal.alarm(5)\n",
    "         try:            \n",
    "            fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\n",
    "            ax.plot([idx for idx,j in enumerate(data[col])],np.array(data[col]))\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_title('Autogenerated Plot' + ' ' + str(idx + 1))\n",
    "            ax.set_facecolor('#ADD8E6')\n",
    "            ax.set_axisbelow(True)\n",
    "            ax.yaxis.grid(color='white', linestyle='-')\n",
    "            ax.xaxis.grid(color='white', linestyle='-')\n",
    "            fig.savefig(data_folder + '/Plots/' + filename + '_' + 'Plot-' + str(idx + 1) + '.png')\n",
    "            plt.close('all')\n",
    "         except:\n",
    "            plt.close('all')\n",
    "            failed2[idx] = True\n",
    "            pass\n",
    "         else:\n",
    "            signal.alarm(0)\n",
    "   \n",
    "   if all(failed1) and all(failed2):\n",
    "      filename = data_path.split(sep='/')[-1]\n",
    "      failed_to_process = open(data_folder + '/failed_to_process.txt','a')\n",
    "      e = datetime.now()\n",
    "      failed_to_process.write(f'{filename} failed to process both histograms and plots on {str(e.year)}, {str(e.month)}, {str(e.day)}\\n')\n",
    "      failed_to_process.close()\n",
    "      return\n",
    "   elif all(failed1) and not all(failed2):\n",
    "      filename = data_path.split(sep='/')[-1]\n",
    "      failed_to_process = open(data_folder + '/failed_to_process.txt','a')\n",
    "      e = datetime.now()\n",
    "      failed_to_process.write(f'{filename} failed to process histograms on {str(e.year)}, {str(e.month)}, {str(e.day)}\\n')\n",
    "      failed_to_process.close()\n",
    "      return\n",
    "   elif all(failed2) and not all(failed1):\n",
    "      filename = data_path.split(sep='/')[-1]\n",
    "      failed_to_process = open(data_folder + '/failed_to_process.txt','a')\n",
    "      e = datetime.now()\n",
    "      failed_to_process.write(f'{filename} failed to process plots on {str(e.year)}, {str(e.month)}, {str(e.day)}\\n')\n",
    "      failed_to_process.close()\n",
    "      return\n",
    "   return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "   if run:\n",
    "      if not connected_to_internet():\n",
    "         return\n",
    "\n",
    "      # Check if data folder exists\n",
    "      data_folder_path = sys.path[0] + '/Data'\n",
    "      if not os.path.isdir(data_folder_path):\n",
    "         print('Directory \"Data\" does not exist in current directory. Creating Directory.')\n",
    "         os.mkdir(path=data_folder_path)\n",
    "         os.mkdir(path=temp_folder)\n",
    "\n",
    "      # Check if websites.csv exists\n",
    "      websites_csv_path = sys.path[0] + '/websites.csv'\n",
    "      if not os.path.isfile(websites_csv_path):\n",
    "         sys.exit('Necessary file \"websites.csv\" does not exist in current directory. Exiting Program.')\n",
    "\n",
    "      # 1. Read in csv with websites\n",
    "      try:\n",
    "         df = pd.read_csv(websites_csv_path,header=0)\n",
    "         df = df.reset_index(drop=True)\n",
    "      except:\n",
    "         sys.exit('Failed to open websites.csv. Exiting Program.')\n",
    "\n",
    "      for idx,info in df.iterrows():\n",
    "\n",
    "         # 2: Iterate over all entries to check if enough time has passed\n",
    "\n",
    "         if ((round(time.time(),0) - info[3]) >= 50):\n",
    "            df.iloc[idx,3] = int(time.time())\n",
    "            dl_folder = data_folder_path + '/' + info[0]    \n",
    "                    \n",
    "            if (info[4] == 'empty') or (not os.path.isdir(dl_folder)):\n",
    "               try:\n",
    "                  if not os.path.isdir(dl_folder):\n",
    "                     os.mkdir(dl_folder)\n",
    "                  filepath = download_url(url=info[1],save_path=dl_folder,type=info[2])\n",
    "                  df.iloc[idx,4] = filepath\n",
    "                     \n",
    "                  autoprocess(filepath,dl_folder)\n",
    "                  try:\n",
    "                     del filepath\n",
    "                  except:\n",
    "                     pass\n",
    "               except:\n",
    "                  pass\n",
    "\n",
    "            elif (info[4] != 'empty') and ((os.path.isfile(info[4])) or (os.path.isdir(info[4]))):\n",
    "               try:\n",
    "\n",
    "                  #1: Download data to temp folder using url, return temp filepath and name of file\n",
    "                  new_filepath = download_url(url=info[1],save_path=temp_folder,type=info[2])\n",
    "                  old_filepath = info[4]\n",
    "\n",
    "                  #1.5: Check to see if files are the same\n",
    "                  if (info[2] == 'zip'):\n",
    "                     same_file = filecmp.cmpfiles(a=new_filepath,b=old_filepath,shallow=False)\n",
    "                  else:\n",
    "                     same_file = filecmp.cmp(f1=new_filepath,f2=old_filepath,shallow=False)\n",
    "\n",
    "                  #2: If files are the same, clear temp folder\n",
    "                  if same_file:\n",
    "                     clear_temp()\n",
    "\n",
    "                  #3: If files are different, move new file in temp to overwrite old file\n",
    "                  #   clear temp folder, delete old data folder, and process new data\n",
    "                  elif not same_file:\n",
    "                     shutil.move(src=new_filepath,dst=old_filepath)\n",
    "                     clear_temp()\n",
    "\n",
    "                     filename = old_filepath.split(sep='/')[-1].split(sep='.')[0]\n",
    "                     data_folder = dl_folder + '/' + filename.split(sep='.')[0] + '_Data'\n",
    "                     shutil.rmtree(path=data_folder)\n",
    "\n",
    "                     autoprocess(data_path=old_filepath,dl_folder=dl_folder)\n",
    "\n",
    "                     try:\n",
    "                        del old_filepath,new_filepath\n",
    "                     except:\n",
    "                        pass\n",
    "\n",
    "               except:\n",
    "                  pass\n",
    "\n",
    "         try:\n",
    "            clear_temp()\n",
    "         except:\n",
    "            pass\n",
    "      \n",
    "      # 3. Check to see if user asked for entry to be deleted <- Not sure if this will get implemented\n",
    "\n",
    "      # 4. Overwrite file \n",
    "      df.to_csv(sys.path[0] + '/websites.csv',index=False)\n",
    "      # print('looped')\n",
    "      \n",
    "   window.after(1, main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Tk()\n",
    "window.title('Transportation Data Manager')\n",
    "window.iconbitmap(sys.path[0] + '/Resources/car2.ico')\n",
    "\n",
    "frame = Frame(window)\n",
    "frame.pack()\n",
    "\n",
    "canvas = Canvas(frame, width=400, height=300, bg='#D3D3D3')\n",
    "canvas.pack()\n",
    "\n",
    "global run\n",
    "run = False\n",
    "\n",
    "start_label = Text(canvas,wrap=WORD,width=30,height=2,padx=6,pady=5,highlightthickness=0)\n",
    "start_label.tag_configure('center',justify='center')  \n",
    "start_label.insert('1.0','When pressed, this button will start the loop')\n",
    "start_label.tag_add('center',1.0,'end')\n",
    "start_label.place(relx = 0.3, rely = 0.4,anchor=CENTER)\n",
    "\n",
    "start_button = Button(canvas, text=\"Start\", command=on_start, padx=6,pady=5,highlightthickness=0)\n",
    "start_button.place(relx=0.75, rely=0.4, anchor=CENTER)\n",
    "\n",
    "end_label = Text(canvas,wrap=WORD,width=30,height=2,padx=6,pady=5,highlightthickness=0)\n",
    "end_label.tag_configure('center',justify='center')  \n",
    "end_label.insert('1.0','When pressed, this button will end the loop')\n",
    "end_label.tag_add('center',1.0,'end')\n",
    "end_label.place(relx = 0.3, rely = 0.6,anchor=CENTER)\n",
    "\n",
    "end_button = Button(canvas, text=\"Stop\", command=on_stop,padx=6,pady=5,highlightthickness=0)\n",
    "end_button.place(relx=0.75,rely=0.6,anchor=CENTER)\n",
    "\n",
    "info_label = Text(canvas,wrap=WORD,width=30,height=5,padx=6,pady=5,highlightthickness=0)\n",
    "info_label.tag_configure('center',justify='center')  \n",
    "info_label.insert('1.0','''This rudimentary GUI controls the script. \n",
    "New buttons and features may be added later if I can make it work''')\n",
    "info_label.tag_add('center',1.0,'end')\n",
    "info_label.place(relx=0.5, rely = 0.15,anchor=CENTER)\n",
    "\n",
    "resized_img = Image.open(sys.path[0] + '/Resources/UT_logo.png').resize((130,100),Image.LANCZOS);\n",
    "img = ImageTk.PhotoImage(resized_img)\n",
    "canvas.create_image(350,260,image=img)\n",
    "\n",
    "who_made_this = Text(canvas,wrap=WORD,width=35,height=3,padx=6,pady=5,highlightthickness=0)\n",
    "who_made_this.tag_configure('center',justify='center')  \n",
    "who_made_this.insert('1.0','''This program was made by Collin Dobson for the UTORII SMaRT internship''')\n",
    "who_made_this.tag_add('center',1.0,'end')\n",
    "who_made_this.place(relx=0.35,rely = 0.85,anchor=CENTER)\n",
    "\n",
    "window.after(1, main)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
